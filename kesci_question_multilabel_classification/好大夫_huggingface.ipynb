{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import collections\n",
    "import logging\n",
    "import os\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForMultipleChoice,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import default_data_collator\n",
    "from transformers.tokenization_utils_base import PaddingStrategy, PreTrainedTokenizerBase\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\n",
    "train_df = pd.read_excel('data/好大夫在线_非标准化疾病诉求的简单分诊数据集/train.xlsx', engine='openpyxl')\n",
    "test_df = pd.read_excel('data/好大夫在线_非标准化疾病诉求的简单分诊数据集/test.xlsx', engine='openpyxl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df['text'] = str(df['title']) + str(df['conditionDesc']) + str(df['hopeHelp']) + str(df['diseaseName'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train_df[0:100].to_csv(\"data/好大夫在线_非标准化疾病诉求的简单分诊数据集/new_train.csv\", index=False)\n",
    "train_df[100:200].to_csv(\"data/好大夫在线_非标准化疾病诉求的简单分诊数据集/new_valid.csv\", index=False)\n",
    "train_df[200:300].to_csv(\"data/好大夫在线_非标准化疾病诉求的简单分诊数据集/new_test.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "data_files = {}\n",
    "data_files[\"train\"] = 'data/好大夫在线_非标准化疾病诉求的简单分诊数据集/new_train.csv'\n",
    "data_files[\"validation\"] = 'data/好大夫在线_非标准化疾病诉求的简单分诊数据集/new_valid.csv'\n",
    "data_files[\"test\"] = 'data/好大夫在线_非标准化疾病诉求的简单分诊数据集/new_test.csv'\n",
    "\n",
    "extension = 'csv'\n",
    "datasets = load_dataset(extension, data_files=data_files)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-8317c29feb5bba8d\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /Users/derbysofti81/.cache/huggingface/datasets/csv/default-8317c29feb5bba8d/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "                            "
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset csv downloaded and prepared to /Users/derbysofti81/.cache/huggingface/datasets/csv/default-8317c29feb5bba8d/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "datasets"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'age', 'diseaseName', 'conditionDesc', 'title', 'hopeHelp', 'label', 'text'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'age', 'diseaseName', 'conditionDesc', 'title', 'hopeHelp', 'label', 'text'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'age', 'diseaseName', 'conditionDesc', 'title', 'hopeHelp', 'label', 'text'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# model_checkpoint = \"bert-base-chinese\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "# https://medium.com/data-folks-indonesia/modeling-using-hugging-face-transformers-f956bccf7ccd\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"bert-base-chinese\",\n",
    "    num_labels=10,\n",
    "    finetuning_task='haodaifu',\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"bert-base-chinese\"\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_config(\n",
    "    config\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /Users/derbysofti81/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": \"haodaifu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /Users/derbysofti81/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt from cache at /Users/derbysofti81/.cache/huggingface/transformers/36acdf4f3edf0a14ffb2b2c68ba47e93abd9448825202377ddb16dae8114fe07.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\n",
      "loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer.json from cache at /Users/derbysofti81/.cache/huggingface/transformers/7e23f4e1f58f867d672f84d9a459826e41cea3be6d0fe62502ddce9920f57e48.4495f7812b44ff0568ce7c4ff3fdbb2bac5eaf330440ffa30f46893bf749184d\n",
      "loading file https://huggingface.co/bert-base-chinese/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-chinese/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer_config.json from cache at /Users/derbysofti81/.cache/huggingface/transformers/2dc6085404c55008ba7fc09ab7483ef3f0a4ca2496ccee0cdbf51c2b5d529dff.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "context_name = \"text\"\n",
    "max_seq_length = 512\n",
    "def preprocess_function(examples):\n",
    "    # 生成模型需要的输入格式 formatting\n",
    "    sentences = examples[context_name]\n",
    "    # Tokenize\n",
    "    tokenized_examples = tokenizer(\n",
    "        sentences,\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\" if 1024 else False,\n",
    "    )\n",
    "    # Un-flatten\n",
    "    return {k: v for k, v in tokenized_examples.items()}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "tokenized_datasets = datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=2,\n",
    "    # load_from_cache_file=not data_args.overwrite_cache,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " #0: 100%|██████████| 1/1 [00:00<00:00, 11.28ba/s]\n",
      " #1: 100%|██████████| 1/1 [00:00<00:00, 10.79ba/s]\n",
      " #0: 100%|██████████| 1/1 [00:00<00:00,  6.44ba/s]\n",
      " #1: 100%|██████████| 1/1 [00:00<00:00,  5.94ba/s]\n",
      " #0: 100%|██████████| 1/1 [00:00<00:00, 11.05ba/s]\n",
      " #1: 100%|██████████| 1/1 [00:00<00:00, 11.65ba/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "tokenized_datasets"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['age', 'attention_mask', 'conditionDesc', 'diseaseName', 'hopeHelp', 'id', 'input_ids', 'label', 'text', 'title', 'token_type_ids'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['age', 'attention_mask', 'conditionDesc', 'diseaseName', 'hopeHelp', 'id', 'input_ids', 'label', 'text', 'title', 'token_type_ids'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['age', 'attention_mask', 'conditionDesc', 'diseaseName', 'hopeHelp', 'id', 'input_ids', 'label', 'text', 'title', 'token_type_ids'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "@dataclass\n",
    "class DataCollatorForSentClassification:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        flattened_features = features\n",
    "        \n",
    "        # padding\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # Un-flatten\n",
    "        batch = {k: v.view(batch_size, -1) for k, v in batch.items()}\n",
    "        # Add back labels\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        # print(batch['input_ids'].shape)\n",
    "        return batch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Data collator\n",
    "data_collator = (\n",
    "  DataCollatorForSentClassification(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# dcsc = DataCollatorForSentClassification(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
    "# dcsc(tokenized_datasets[\"train\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "batch_size = 8\n",
    "training_args = TrainingArguments(\n",
    "    \"saved_model/haodaifu\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# Metric\n",
    "def compute_metrics(eval_predictions):\n",
    "    predictions, label_ids = eval_predictions\n",
    "    # print(predictions, label_ids)\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (preds == label_ids).astype(np.float32).mean().item()}\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"] if training_args.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id, title, hopeHelp, age, diseaseName, conditionDesc.\n",
      "***** Running training *****\n",
      "  Num examples = 100\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 39\n",
      "\n",
      "\u001b[A"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "3efdbfa231e238790044067e0ee7096fb542fcb4c99101a76533e640110a3d90"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}